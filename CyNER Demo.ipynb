{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95beae09",
   "metadata": {},
   "source": [
    "#### Train(finetune) transformer on user provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef41568a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 03:21:38 INFO     *** initialize network ***\n",
      "2023-05-27 03:21:39 INFO     create new checkpoint\n",
      "2023-05-27 03:21:39 INFO     checkpoint: .ckpt\n",
      "2023-05-27 03:21:39 INFO      - [arg] dataset: dataset/mitre\n",
      "2023-05-27 03:21:39 INFO      - [arg] transformers_model: xlm-roberta-base\n",
      "2023-05-27 03:21:39 INFO      - [arg] random_seed: 1\n",
      "2023-05-27 03:21:39 INFO      - [arg] lr: 5e-06\n",
      "2023-05-27 03:21:39 INFO      - [arg] epochs: 20\n",
      "2023-05-27 03:21:39 INFO      - [arg] warmup_step: 0\n",
      "2023-05-27 03:21:39 INFO      - [arg] weight_decay: 1e-07\n",
      "2023-05-27 03:21:39 INFO      - [arg] batch_size: 32\n",
      "2023-05-27 03:21:39 INFO      - [arg] max_seq_length: 128\n",
      "2023-05-27 03:21:39 INFO      - [arg] fp16: False\n",
      "2023-05-27 03:21:39 INFO      - [arg] max_grad_norm: 1\n",
      "2023-05-27 03:21:39 INFO      - [arg] lower_case: False\n",
      "2023-05-27 03:21:39 INFO     target dataset: ['dataset/mitre']\n",
      "2023-05-27 03:21:39 INFO     data_name: dataset/mitre\n",
      "2023-05-27 03:21:39 INFO     formatting custom dataset from dataset/mitre\n",
      "2023-05-27 03:21:39 INFO     found following files: {'train': 'train.txt', 'valid': 'valid.txt', 'test': 'test.txt'}\n",
      "2023-05-27 03:21:39 INFO     note that files should be named as either `valid.txt`, `test.txt`, or `train.txt` \n",
      "2023-05-27 03:21:39 INFO     dataset dataset/mitre/train.txt: 2810 entries\n",
      "2023-05-27 03:21:39 INFO     dataset dataset/mitre/valid.txt: 812 entries\n",
      "2023-05-27 03:21:39 INFO     dataset dataset/mitre/test.txt: 747 entries\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/userHome/userhome1/hanyong/miniconda3/envs/cyner/lib/python3.7/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "2023-05-27 03:21:50 INFO     using `torch.nn.DataParallel`\n",
      "2023-05-27 03:21:50 INFO     running on 3 GPUs\n",
      "2023-05-27 03:21:53 INFO     *** start training from step 0, epoch 1 ***\n",
      "/userHome/userhome1/hanyong/miniconda3/envs/cyner/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "2023-05-27 03:21:58 INFO     [epoch 1] * (training step 0) loss: 3.300, lr: 0.00000500\n",
      "2023-05-27 03:22:03 INFO     *** KeyboardInterrupt ***\n",
      "2023-05-27 03:22:03 INFO     [training completed, 9.421810388565063 sec in total]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataParallel' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_246033/1002712102.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         'max_seq_length': 128}\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcyner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformersNER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CyNER/cyner/transformers_ner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m                                         cache_dir=cache_dir)\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CyNER/cyner/tner/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, monitor_validation, batch_size_validation, max_seq_length_validation)\u001b[0m\n\u001b[1;32m    443\u001b[0m         )\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbest_f1_score\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cyner/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1270\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataParallel' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'\n",
    "\n",
    "import cyner\n",
    "\n",
    "cfg = {'checkpoint_dir': '.ckpt',\n",
    "        'dataset': 'dataset/mitre',\n",
    "        'transformers_model': 'xlm-roberta-large',\n",
    "        'lr': 5e-6,\n",
    "        'epochs': 20,\n",
    "        'max_seq_length': 128,\n",
    "        'batch_size': 128}\n",
    "model = cyner.TransformersNER(cfg)\n",
    "model.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0343121",
   "metadata": {},
   "source": [
    "#### Import CyNER and get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cyner\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "with open('sample_text.txt') as f:\n",
    "    text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4749a612",
   "metadata": {},
   "source": [
    "#### Model1: Only using pretrained transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3766f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = cyner.CyNER(transformer_model='.ckpt', use_heuristic=False, flair_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f94a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = model1.get_entities(text)\n",
    "\n",
    "for e in entities:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fcee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3edc170",
   "metadata": {},
   "source": [
    "#### Model2: Using pretrained transformers and heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94605d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = cyner.CyNER(transformer_model='.ckpt', use_heuristic=True, flair_model=None, priority='HTFS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = model2.get_entities(text)\n",
    "\n",
    "for e in entities:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dce0f612",
   "metadata": {},
   "source": [
    "#### Model3:  Using pretrained transformers with heuristics and Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c393f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = cyner.CyNER(transformer_model='.ckpt', use_heuristic=True, flair_model='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ab089",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = model3.get_entities(text)\n",
    "\n",
    "for e in entities:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f90626",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb054956",
   "metadata": {},
   "source": [
    "#### Model4:  Using pretrained transformers with heuristics and Spicy and Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = cyner.CyNER(transformer_model='.ckpt', use_heuristic=True, spacy_model='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a79024",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = model4.get_entities(text)\n",
    "\n",
    "for e in entities:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
